\begin{abstract}

There has been a growing interest in developing machine learning (ML)
models for\XComment{ code summarization} code comprehension and
generation tasks, including \comgen, code generation, and \methnam.
%Sophisticated models, based on advanced ML, such as deep neural
%networks, graph neural networks, and transformers led to high values
%for automatic metrics, e.g., \bleu, commonly used in evaluations.
Despite substantial progress on the models' effectiveness, the
evaluation \methodologies, i.e., the way we split datasets into
training, validation, and testing, were (likely) misleading.
%insufficiently described.
Specifically, no prior work on the aforementioned topics took into
account the timestamps for code elements and natural language elements
(e.g., methods and comments) when splitting a dataset into the
training, validation, and testing sets.  This led to evaluations where
the ML models trained on future knowledge are applied on old elements;
%% that had elements in a testing set with timestamps larger than
%% elements in a training set;
we refer to this as the time\XComment{ inversion} leakage.
%
We ask if the time\XComment{ inversion} leakage is an issue and argue
that it might not be, but it depends on the intended usages of the
model.
%
Thus, we call the community to define use cases (reflecting the way an
ML model would be used in practice) for all newly developed ML models.
Depending on the use case, one should choose an appropriate evaluation
\methodology (in which case even the time\XComment{inversion} leakage
might not be an issue for some use cases).
%
We define and formalize three use cases and evaluation \methodologies
in this paper.
%
%We perform an experiment to evaluate the impact of
One of the novel use cases (which might be the most practical)
requires that training, validation, and testing sets are \evoaware,
i.e., any example in the training set has the timestamp that is
smaller than any example in the testing set; this has not been done
in the past.
%
We then perform an experiment to evaluate the impact of choosing one
of the use cases for several recently proposed ML models.
% for \comgen and \methnam.
Our results show that the performance of the models vary widely
depending on the use case.  Moreover, we find that some conclusions
change depending on the use case.

%% experiments in
%% which elements that ended up in a testing set could have a timestamp
%% that is larger than the timestamp for elements that ended up in a
%% training set.  We call this past and future inversion.

% of code elements and natural language elements when splitting a
%dataset into training, validation, and testing sets.

%led to good results, measured in terms of automatic metrics, e.g.,
%\bleu.  Despite the progress, it remains unclear what current
%evaluation \methodologies correspond to in practice.

%% Generating natural language descriptions for source code a.k.a. code
%% summarization is important.  Recent work built many neural-network
%% based techniques trained and evaluated on big code \datasets.  While
%% the \datasets are usually taken from code repositories, e.g., served
%% on GitHub, no prior work has taken advantage of the rich
%% \textit{evolution} history of the repositories for either training
%% or evaluation.  In this paper, we show that evolution-aware code
%% summarization techniques significantly improve against their
%% non-evolution-aware counterparts by replicating and comparing them
%% on a novel evolution-aware code summarization \dataset.  Moreover,
%% we evaluate the robustness of code summarization techniques against
%% adversarial attacks on our \dataset.
%%
% We also formalize a novel task of suggesting high quality commits
% and build a baseline and a neural-network based technique utilizing
% our \dataset.
\end{abstract}
