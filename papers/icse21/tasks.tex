\section{Tasks}
\label{sec:tasks}

This section describes the two tasks in our study: \comgen and
\methnam.  For each task, we (1) give a brief summary of the
background, (2) select several recent well-studied machine learning
models to experiment with, and (3) describe the automatic metrics for
the task.

\subsection{\ComGen}
\label{sec:tasks:comgen}

\MyPara{Background} Developers frequently write comments in natural
language together with their code to deliver messages to their users
(e.g., via API comments) and to communicate among themselves (e.g.,
via todo
comments)~\cite{PadioleauETAL09Listening,NieETAL18Natural,PascarellaETAL19Classifying}.
Good comments help developers quickly and precisely comprehend code.
However, writing and maintaining comments can be tedious and
error-prone, making comments easy to be wrong or outdated, which leads
to inconsistencies and
bugs~\cite{TanETAL07Icomment,TanETAL12TComment,RatolAndRobillard17Detecting}.
The task of \comgen tries to automatically generate comments from
code.  So far, most related work~\cite{HuETAL18Deep,HuETAL19Deep,XXX}
focused on generating API comments (e.g., \javadoc summaries) from
methods.

\MyPara{Models} We select three models: \UseMacro{CG-Seq2seq},
\UseMacro{CG-Seq2seqAtt}, and \UseMacro{CG-DeepCom}.  All three models
formulate the \comgen task as a machine translation problem, and
exploits encoder-decoder neural networks to solve it.

\UseMacro{CG-Seq2seq} uses a vanilla encoder-decoder neural network,
where both the encoder and the decoder are single-layer GRU recurrent
neural network~\cite{ChoETAL14Learning}.  This model is usually used
as a baseline
%to compare against
in prior work based on deep learning. \Fix{You need to cite the prior work here}

\UseMacro{CG-Seq2seqAtt} is an upgraded version of
\UseMacro{CG-Seq2seq} that is altered with the attention mechanism
(specifically, the global attention model proposed in Luong et
al.~\cite{LuongETAL15Effective}).  The attention mechanism improves
the encoder-decoder neural network's ability in capturing long-range
dependencies among input tokens.

\UseMacro{CG-DeepCom}, recently proposed by Hu et
al.~\cite{HuETAL19Deep}, is a state-of-the-art deep learning model for
\comgen.  It exploits an encoder-decoder neural network that combines
both lexical and structural inputs: the code tokens and the AST
(abstract syntax tree) tokens, by using two encoders rather than one.
Figure~\ref{X} illustrates the architecture of this model.  The tree
structure in the AST is sequentialized by performing a novel
``structure-based'' traversal.  It also employs the attention
mechanism for both inputs.

\MyPara{Automated metrics} We use two automated metrics: average
sentence-level \bleu-4~\cite{XXX} and \xmatch. The \xmatch is the
percentage of examples for which the model's prediction is exactly the
same with reference comments.  \bleu is one of the most popular
language generation metrics which produces the score based on matches
of 4-grams of words between the reference and prediction.\Fix{maybe an example?}

%% # Comment generation
%% - Use cases: evolution, mixed-project, cross-project;
%% - Models: DeepCom-Hybrid, Seq2seq, Seq2seqAtt;
%% - 3 use cases * 3 models * 3 trials = 27 jobs to run

\subsection{\MethNam}
\label{sec:tasks:methnam}

\MyPara{Background} Descriptive and conventional names for code
elements (variables, methods, classes, etc.) are a vital part of
readable and maintainable code~\cite{XXX}.  Naming methods is
particularly hard and important, because the names need to be both
concise, usually containing only a few tokens and being much shorter
than the methods they are summarizing, and comprehensible, such that
they deliver the key functionality of the
code~\cite{LawrieETAL06Whats}.  Prior work explored various approaches
to solve the task, including information retrieval~\cite{XXX},
statistical~\cite{XXX}, and deep
learning~\cite{AllamanisETAL16Convolutional, XuETAL19Method,
  YonaiETAL19Mercem, AlonETAL19Code2vec, AlonETAL19code2seq,
  NguyenETAL20Suggesting}.

\MyPara{Models} We selected several recent deep learning based models
for this task:\UseMacro{MN-Bi-LSTM}, \UseMacro{MN-no-split-Bi-LSTM},
\UseMacro{MN-Code2Seq}.  They all formulate the task as a machine
translation problem from code to method names, and solve that using encoder-decoder neural
networks.

\UseMacro{MN-Bi-LSTM} uses bidirectional LSTM-based encoder-decoder
architecture with global attention. It takes method body as input and
outputs the target method names tokens while decoding.

\UseMacro{MN-no-split-Bi-LSTM} shares the same architecture as
\UseMacro{MN-Bi-LSTM} except that the tokens in the input method body
and target sequences are not split into subtokens.

\Fix{Any prior work you need to cite for the above two methods?}

\UseMacro{MN-Code2Seq}, proposed by Alon et
al.~\cite{AlonETAL19code2seq}, leverages the syntactic structure of
programming languages by extracting paths in abstract syntax tree
(AST) to represent a code snippet.Each path is encoded into a
fixed-length vector using LSTMs. Global attention is applied to all
the paths during decoding to generate the target method names.

\MyPara{Automated metrics} In addition to \xmatch, we adopt the
automated metrics: \fone, \precision, \recall used by Allamanis et
al.~\cite{AllamanisETAL16Convolutional} and Alon et
al. ~\cite{AlonETAL19code2seq} to measure models' performance.

%% # Method name generation
%% - Use cases: evolution, mixed-project, cross-project;
%% - Models: Code2Seq, BiLSTM, BiLSTM(no-split);
%%   // Ideally add Transformer, in which case BiLSTM(no-split) can be removed
%% - 3 use cases * 3 models * 3 trials = 27 jobs to run




%% ---------- below are OLD design before Aug 22, 2020 ----------

%% We define \textit{evolution-aware} evaluation setting as following.
%% %
%% We take $k$ snapshots of a evolving \dataset $\{S_1, S_2, \dots,
%% S_k\}$ at time points $\{t_1, t_2, \dots, t_k\}$ respectively in
%% chronological order.
%% %
%% To evaluate a ML technique on $S_{i+1}$, we only train the technique
%% on the data available before $t_{i+1}$, i.e., $\{S_1, \dots, S_i\}$.
%% %
%% We define task-specific filter functions $F_j(S_{i+1}, S_i)$ to obtain
%% subsets of $S_{i+1}$ for evaluation in different settings; for
%% example, removing data points that already existed in $S_i$ to perform
%% an ``unbiased'' (in ML view) evaluation.

%% Given the project data available at $k$ different time points $\{t_1,
%% \dots, t_k\}$, at each time point $t_i$, we train our system on
%% all the data available at time $t_i$ then test on the data existing at
%% time $t_{i+1}$ which are filtered by the task specific filter function
%% $F$.

%% In contrast, we define \textit{evolution-unaware} evaluation setting
%% as randomly separating the data points in $S_k$ into three subsets:
%% $S_{train}$, $S_{val}$, $S_{test}$.  Then, we train the technique on
%% $S_{train}$ with $F(S_{val}, S_{train})$ as validation set, and
%% evaluate the trained technique on $F(S_{test}, S_{train} \sunion
%% S_{val}$.

%% Our goal is to evaluate the same set of machine learning techniques
%% using both evolution-aware setting and evolution-unaware setting to
%% testify whether the observations made in evolution-unaware setting are
%% still valid in evolution-aware setting.

%% \subsection{Comment Generation}

%% \subsubsection{Filter Functions}

%% We evaluate in three settings using three different filter functions.

%% $F_a$ will remove all the methods in $S_{i+1}$ that have same
%% signature with some methods in $S_i$. Here, signature is the tuple of
%% (class name [including outer class name if in an inner class], method
%% name, argument types).  Since most comment generation models only take
%% method snippets (method name, return type, arguments and method body)
%% as input, the filter function ensures the methods in $S_{i+1}$ have
%% not been seen by the model during training.  This also ensures moving
%% files between packages do not result in ``new'' methods.

%% $F_b$ will filter all the methods that have same signature and same
%% method body with some methods in $S_i$, i.e., include the new methods
%% and methods that have been updated between two time points.

%% $F_c$ will filter all the methods in time $S_{i+1}$ that have same
%% signature, same method body and same fully-qualified class name with
%% some methods in $S_i$ to avoid data leakage, i.e., include new
%% methods, updated methods and the methods that are simply copied or
%% moved from one file to another.

%% We denote the subsets after filtering as:

%% \begin{flalign*}
%%   S^a_{i+1} &= F_a(S_{i+1}, S_i) \\
%%   S^b_{i+1} &= F_b(S_{i+1}, S_i) \\
%%   S^c_{i+1} &= F_c(S_{i+1}, S_i)
%% \end{flalign*}

%% Note that $S^a_{i+1} \subseteq S^b_{i+1} \subseteq
%% S^c_{i+1} \subseteq S_{i+1}$.


%% \subsubsection{Data}

%% We collected the \dataset of comment-code pairs from the same
%% repositories in \citet{HuETAL18Deep}.
%% We take the snapshots of the \dataset at these time points:

%% \begin{flalign*}
%%   t_1 = \text{Jan 1 2013}&,\quad t_2 = \text{Jan 1 2014}, \\
%%   t_3 = \text{Jan 1 2015}&,\quad t_4 = \text{Jan 1 2016}, \\
%%   t_5 = \text{Jan 1 2017}&,\quad t_6 = \text{Jan 1 2018}, \\
%%   t_7 = \text{Jan 1 2019}&,\quad t_8 = \text{Jan 1 2020}
%% \end{flalign*}

%% We consider the type of experiment of training the technique on the
%% first year's data with the second year's data as validation set, and
%% testing on the third year's data.  This setting assumes that new
%% comments should follow similar style of recently-written comments.  We
%% perform 5 groups of experiments:

%% \begin{enumerate}
%% \item Train on $S^a_2$ with $S^a_3$ as validation set, and
%%   test on $S^{a,b,c}_4$.
%% \item Train on $S^a_3$ with $S^a_4$ as validation set, and
%%   test on $S^{a,b,c}_5$.
%% \item Train on $S^a_4$ with $S^a_5$ as validation set, and
%%   test on $S^{a,b,c}_6$.
%% \item Train on $S^a_5$ with $S^a_6$ as validation set, and
%%   test on $S^{a,b,c}_7$.
%% \item Train on $S^a_6$ with $S^a_7$ as validation set, and
%%   test on $S^{a,b,c}_8$.
%% \end{enumerate}

%% An alternative type of experiment would be training on all data before
%% a time point with the next year's data as validation set, and testing
%% on the following next year's data.  For example, train on $S_1$ with
%% $S^a_2$ as validation set, and test on $S^a_3$.  However, in
%% this setting, obsolete comments may dominate the training set and thus
%% bias the predictions of techniques.

%% Different from DeepCom's data processing procedure, we do not
%% filtering getter, setter, or constructor methods, where our main
%% consideration is to focus on the comparison of different evaluation
%% setting and avoid any bias that might be introduced by this additional
%% step.

%% \subsubsection{Techniques}

%% Hybrid-DeepCom, DeepCom(SBT), DeepCom(Pre-order), Seq2Seq(Attention), Seq2Seq.


%% \subsection{Method Name Generation}

%% \subsubsection{Filtering Functions}
%% $F_a$ will remove all the methods in $S_{i+1}$ that have same
%% method name, class name [including outer class name if in an inner
%%   class] and method body with some methods in $S_i$.
%% \subsubsection{Data}

%% \subsubsection{Techniques}

%% Code2seq, 2-layer BiLSTM(no token splitting), 2-layer BiLSTM,
%% potentially code2vec.
