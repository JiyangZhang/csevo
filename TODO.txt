TASKS

 - For all models: only save the latest X checkpoints, X is the early
   stopping threshold.

 - DeepCom (other comment generation models might be using the same
   data):
   - Comment should be sub-tokenized;
   - In comment: { @ X Y } -> Y, where X = code, link, linkplain, etc.;
   - Fix wired tokenization: "x_y" -> "x_ y", should be "x _ y";
   - In comment: < code > X < /code > -> X;
   - (after previous step) In comment: < X > -> nothing, where X = pre, code, ... (other html tags);

 - DeepCom: the test_common likely has bug, the output of all examples
   are almost the same: "if the current value of the specified .";
   This doesn't happen in valid.X.out.

 - Plot learning curve from the models.

 - [Pengyu] Paper: change old dataset tables (evolution 2017-2020)
   into figures, add to data section.

 - [Pengyu] Add highlight for best metric in results tables.

 - The experiment for comment generation {DeepCom, Seq2seq,
   Seq2seqAtt} models used problematic dataset, because every time
   DeepComDataProcessor runs, it doesn't remove existing data but
   simply add more data onto it.  So I was running on the
   1000_projects_dataset+10_projects_dataset...

 - We wanted to collect 1000 projects but only got 880 projects.  The
   inspection into random 10 projects (out of the missing 120) showed
   that most of them should have some data collected, so maybe there
   was some bug at the time of data collection.  Recollect those data
   when we get a chance.

 - We're reusing the comment generation dataset for the method naming
   task, but that dataset only contains methods with comments.
   Collect another dataset that also contains methods without comments
   when we get a chance.

 - Another evaluation setting: training on t_0~t_{n-2} with validation
   on t_{n-2}~t_{n-1}, and evaluating on t_{n-1}~t_n

 - Another (fun) evaluation setting: training on t_{n-1}~t_n with
   validation on t_{n-2}~t_{n-1}, and evaluating on t_{n-3}~t_{n-2}


========================================

OLD

 - We can use PRs (both those that are accepted and rejected) in our
   evaluation.  We can apply our approach to each PR and evaluate the
   outcome (and even see if that correlates with PR being
   rejected/accepted).

 - Add into training log: training_time, num_data.

 - Add BLEU in TaccRunner.

 - Add BLEU as metric in the Code2Seq source code.
 
 - Look at other models implementation: Tree-LSTM-GRU,
   one of {BiLSTM+GNN, code+gnn+BiLSTM}.

 - Check the data that has the same method body but different
   comments.  That can be used for multi-reference evaluation.

 - (Later) finish DataProcessor for ast-attendgru, currently paused
   because the work is too similar to DeepCom

 - Collect the number of stars for each repo, and add that as a
   feature to the models.

 - Move the paper format to ICSE'20 soon (if it's still using
   ACM format).

 - (Later) We already have 499 projects dataset, freezing the
   dataset for now.  Add openjdk_jdk (already processed, but data too
   large and can't fit in Python - maybe using json?).  Move the
   remaining data collection to TACC Stampede2.

 - Before upload our code for submission, check and remove "# TODO
   private info" parts.

 - We only retain non-constructor methods with non-empty Javadoc
   description part. Consider also retain other methods as well when
   we have more time / want to look at them.


========================================

ACCOMPLISHED

 + [Pengyu] Define data format.

 + [Jiyang] Check if any prior work used additional context other than
   just method code and comment - we may need to collect those as
   well.

 + [Pengyu] Initial script+tool for collecting data for 1 project.

 + [Pengyu] Collect and freeze the top-1000 GitHub Java non-fork repo list.

 + [Pengyu] Collect data for first 100 projects.

 + [Jiyang] Finish the table-models.

 + [Pengyu] Dataset splitting

 + [Jiyang] Finish DeepCom data process

 + [Jiyang] Add function to build Vocabulary of code,nl,sbt for DeepCom. 
 
 + [Pengyu] Make the dataset accessible to both of us.

 + [Pengyu] DeepCom: try to start training.

 + Errors while collecting data: openjdk_jdk. Fixed, will be added in
   next version of dataset (on 20200501).

 + [Pengyu] The python->Java part for code2seq processor.

 + [Pengyu] DeepCom: process the latest version data as well. Try to
   speed up by paralleling.

 + [Jiyang] TACC environment to have:
   * projects cloned at $WORK/projects/csevo
   * run update-repos.sh
   * conda environment "csevo", with packages at python/requirements.txt installed
   * conda environment "DeepCom", `conda env create -n DeepCom -f python/envs/DeepCom.yml`

 + [Jiyang] Get DeepCom models running on TACC.

 + [Jiyang] include the JavaExtractor from code2seq to our repo, and
   modify it to take methods rather than raw Java files.

 + Load the method data to the processors in Java.

 + [Pengyu] Filter pure abstract methods before splitting dataset.

 + [Pengyu] Down sample the dataset to 1% to get the small dataset.

 + Change the validation set to follow the testing set - e.g.,
   Evolution-Latest performs training on Evolution set, but
   validation&testing on Latest set (as opposed to currently
   training&validation on Evolution set, and testing on Latest set).

 + [Jiyang] Figure out how to extract learning curve from Code2Seq
   training log.

 + [Jiyang] Get the buggy BLEU score and correct BLEU score.

 + Plot learning curve from log files.

 + Debug the training loss explosion issue.

 + [Jiyang] Add ISSTA 2020 related work to the paper.


 + [Jiyang] Document, formalize the task (filter function, train and
 test), take comment generation as example.

 + [Jiyang] Parse repository and save data from 2020, 2019, 2018,
 2017, 2016

 + [Jiyang] Collect data from 8 time points.

 + [Jiyang] Get stats for the collected data.

 + [Jiyang] Change the code for filtering to concurrent.

 + [Jiyang] Randomly choose 10 repos and get the data.

 + [Jiyang] Run all exps on TACC and get results.

 + [Jiyang] Create filter functions to get dataset.

 + [Jiyang] Not sure how to formalize the description of filter functions.

 + [Jiyang] Download and collect data from 1000 top projects.

 + [Jiyang] Add patience in config file, test whether it can work on
 10 projects.

 + [Jiyang] Modify DeepCom for early stopping

 + [Jiyang] Filter the 1000 data

 + [Jiyang] Dataset statistics for large (1000) dataset.

 + [Jiyang] Prepare Code2seq, filter function for Code2Seq. (Run on 10 small dataset (that is in xps1))

 + [Jiyang] Build environment and run debug dataset for C2s in seoul, run test for code2seq

 + [Jiyang] Collect stats for Debug Code2seq dataset.

 + [Jiyang] Prepare latest data for 1000 projs (Running on Seoul)

 + [Jiyang] Create data for method naming baselines (Use DeepCom code)
  + running: processing data for biLSTM (seoul) (evo + latest)

 + [Jiyang] Implement Bi-LSTM using OpenNMT

 + [Jiyang] Prepare scripts and run train/eval/test using BiLSTM debug.

 + [Jiyang] Fix bug in splitting evo dataset.

 + [Jiyang] DeepCom changes 500 steps per eval. Code2Seq increase Batch size

 + [Jiyang] Use ijson to load large json file from IO stream.

 + [Jiyang] Process latest data to get DeepCom-latest data. (Json file too large need to be solved)

 + [Jiyang] Run Bi-LSTM using debug data and get results. (mixed projects)

 + [Jiyang] Train and get no-split-biLSTM result on seoul
    1314 test, 1415 te
    1516 test, 1617 training

 + [Jiyang] Run Code2seq using debug data and get results.
   Running - Bi-LSTM evo/latest training on seoul
           - Code2seq-debug latest test in seoul

 + [Jiyang] Done: Seq2seq, DeepCom, DeepCom-SBT collected in metrics_dir

 + [Jiyang] Fix Code2Seq eval to lower case.

 + [Jiyang] Collect val results for DeepCom-lat, Seq2seq-lat, DeepCom-SBT-lat, Seq2seqAtt-lat, Seq2seq-evo

 + [Jiyang] Split the data indexes to train/val/test per project.

 + [Jiyang] Split the projects to train/val/test.

 + [Jiyang] Eval script for method name has bug: precision too high.

 + [Jiyang] Change code2seq val script, BiLSTM val script. For TACCRunner
 + Update experiment scripts according to the latest plan.
   + [Pengyu] API change for *Processor.
   + [Pengyu] API change for *Runner.prepare.
   + [Pengyu] API change for *Runner.run.
   + [Pengyu] Comment generation: DeepCom
   + [Pengyu] Comment generation: Seq2Seq
   + [Pengyu] Comment generation: Seq2SeqAtt
   + [Jiyang] Method naming: Code2Seq
   + [Jiyang] Method naming: BiLSTM
   + [Jiyang] Method naming: BiLSTM(no-split)


 + Process large dataset for method naming task: filter, process models' data.

 + Run 10 projects, 1 trial on TACC.
   + [Pengyu] Comment generation: 9 jobs = 3 models * 3 settings * 1 trial
   + [Jiyang] Method name generation: 9 jobs = 3 models * 3 settings * 1 trial

 + [Jiyang] Process large data for code2seq, Bi-LSTM. (Use data of Bi-LSTM to generate data for Bi-LSTM (no split))

 + [Jiyang] Open NMT with pytorch 1.6, has bugs when using multithreading
   possible sol: download openNMT change the code to import numpy first.
   https://github.com/pytorch/pytorch/issues/37377

 + [Jiyang] Paper: fill up Table 1.

 + [Jiyang] Get error-data-ids for method naming tasks' processors.

 + [Pengyu] Update eval scripts to output score per data, with considering of
   error-data-ids.

 + [Pengyu] Update scripts for extracting results and making tables.

 + [Pengyu] statistical significant tests, across models, across
   eval_settings.

 + [Pengyu] Paper: the figures in method.tex.

 + [Pengyu] Paper: add dataset statistics table.

 + [Pengyu] Change all metrics scale to 0~100.

 + [Pengyu&Jiyang] visualize the metrics per example.

 + [Pengyu&Jiyang] visualize the diff of metrics between trials per example.

 + Run 1000 projects, 3 trials on TACC.
   [Pengyu] Comment generation: 27 jobs = 3 models * 3 settings * 3 trials
   - train: 24-48h
   - test_common: 24h
   [Jiyang] Method naming: 27 jobs = 3 models * 3 settings * 3 trials
   - train: 24-48h
   - test_common: 24h

 + Testing the model (esp. test_standard) might take very long time.
   We don't need test_standard.

 + [Jiyang] Remove previous checkpoints before testing for biLSTM,

 + [Jiyang] Move ref file to models-data/dir for CODE2SEQ.

 + [Jiyang] Cut down dat aset size from 1000 projects to 100 projects:
  select the top 100 projects (based on stars) from the previous 1000 projects.

 + [Jiyang] Removing the html tags & "{@inheritadoc}" comments, and perform
   similar filtering as we did in hcomments / comment update
   work. Document what filtering we performed.

 + [Jiyang] adjust the ratio of train:val:test (e.g., to 70:10:20) such that
   test_common is not too small. Should split the projects after
   filtering.

 + [Jiyang] Update the raw data with methods no comments.

 + [Jiyang] Recreate dataset, com-gen and meth-nam different dataset.
  step1 select top 100 projects that have high stars and the numMethod(2020) > 1, do alpha filter to get changes.
  step 2 if com-gen: clean data from 100 projects (clean javadoc) -> clean-method-id.json
  step 3 split both cross proj and mixed proj (save the random seed for shuffle)
  Output: com-gen-shared, meth-nam-shared folders

 + Rerun existing experiments (1 trial) with 100 projects.
